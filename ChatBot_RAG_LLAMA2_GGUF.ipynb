{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp-Xsk-7l64w"
      },
      "outputs": [],
      "source": [
        "!pip3 install sentence-transformers\n",
        "!pip3 install llama-index\n",
        "!pip3 install llama-index-readers-json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install llama-index-embeddings-huggingface\n",
        "!pip3 install llama-index-llms-llama-cpp"
      ],
      "metadata": {
        "id": "BDMZEQqGmEEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download data.json\n",
        "\n",
        "!curl -f -o data.json https://devapi.beyondchats.com/api/get_message_with_sources"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jasNfDZ4Nv1l",
        "outputId": "d03159c3-fe76-4bf8-bff1-79bcd5add5f8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 46685    0 46685    0     0  37402      0 --:--:--  0:00:01 --:--:-- 37407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.llama_cpp.llama_utils import (\n",
        "    messages_to_prompt,\n",
        "    completion_to_prompt,\n",
        ")\n",
        "import os\n",
        "from llama_index.core.response.notebook_utils import display_response\n",
        "\n",
        "\n",
        "llm = LlamaCPP(\n",
        "    model_url=\"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf\",\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=512,\n",
        "    context_window=4096,\n",
        "    generate_kwargs={},\n",
        "    model_kwargs={\"n_gpu_layers\": -1},\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRxo2TD8nkR3",
        "outputId": "0921d179-205c-4e6d-9248-57e3da3801bc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /tmp/llama_index/models/llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:   65 tensors\n",
            "llama_model_loader: - type q3_K:  160 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2694.32 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_iter = llm.stream_complete(\"Hello\")\n",
        "for response in response_iter:\n",
        "    print(response.delta, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9uCXEHcnkuf",
        "outputId": "47ebf23e-4949-4fc7-9515-801aa1c9e1eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Hello! I'm here to assist you in any way I can. Please provide the specific instructions or questions you have, and I will do my best to help."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =   26398.82 ms\n",
            "llama_print_timings:      sample time =      20.45 ms /    36 runs   (    0.57 ms per token,  1760.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =   26398.60 ms /    66 tokens (  399.98 ms per token,     2.50 tokens per second)\n",
            "llama_print_timings:        eval time =   21003.97 ms /    35 runs   (  600.11 ms per token,     1.67 tokens per second)\n",
            "llama_print_timings:       total time =   47602.11 ms /   101 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor"
      ],
      "metadata": {
        "id": "l2TcTDJUoGqq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.json import JSONReader\n",
        "\n",
        "# Initialize JSONReader\n",
        "reader = JSONReader()\n",
        "\n",
        "# Load data from JSON file\n",
        "documents = reader.load_data(input_file=\"data.json\", extra_info={})\n"
      ],
      "metadata": {
        "id": "grmnHVrRnx17"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "Settings.llm = llm\n",
        "Settings.chunk_size = 256"
      ],
      "metadata": {
        "id": "OL--_StBoJA9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "2DAG0lhroiBr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=1,\n",
        ")"
      ],
      "metadata": {
        "id": "qKep2HI0ojed"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import get_response_synthesizer\n",
        "\n",
        "synth = get_response_synthesizer(streaming=True)\n",
        "\n",
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
        "    response_synthesizer=synth\n",
        ")\n"
      ],
      "metadata": {
        "id": "RrNW-h3volRT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_stream = query_engine.query(\"what is the average cost of a regular IVF cycle?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2vdZfBgonPv",
        "outputId": "ce3309f3-6f44-4feb-d227-2f7a95df36dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text in response_stream.response_gen:\n",
        "    print(text,end=\"\",flush=True)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8OivcucxXgw",
        "outputId": "46727b1d-7bef-4304-9481-2a82bd8cda62"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Based on the provided context information, the average cost of a regular IVF (in vitro fertilization) cycle is approximately Rs 2 to 4 lakhs (around $2,700 to $4,80"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =   26398.82 ms\n",
            "llama_print_timings:      sample time =      28.04 ms /    50 runs   (    0.56 ms per token,  1783.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =  115377.55 ms /   279 tokens (  413.54 ms per token,     2.42 tokens per second)\n",
            "llama_print_timings:        eval time =   30495.33 ms /    49 runs   (  622.35 ms per token,     1.61 tokens per second)\n",
            "llama_print_timings:       total time =  561251.00 ms /   328 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_stream = query_engine.query(\"do you offer online delivery?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHuyJ9jZ72Ra",
        "outputId": "f35fb910-c046-42be-fd22-07c3eff79166"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text in response_stream.response_gen:\n",
        "    print(text,end=\"\",flush=True)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT4x3BFF9UAK",
        "outputId": "19ee295e-208f-4f68-aa64-e4b24e4ff6ae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Yes, we offer online delivery services through major platforms like Swiggy and Zomato. You can also order directly from our website!"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =   26398.82 ms\n",
            "llama_print_timings:      sample time =      17.52 ms /    30 runs   (    0.58 ms per token,  1712.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =  112976.94 ms /   283 tokens (  399.21 ms per token,     2.50 tokens per second)\n",
            "llama_print_timings:        eval time =   17837.00 ms /    29 runs   (  615.07 ms per token,     1.63 tokens per second)\n",
            "llama_print_timings:       total time =  166135.36 ms /   312 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_stream = query_engine.query(\"How can I sign up?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laAB0uyiNLkZ",
        "outputId": "532fb71a-14e1-4939-8d1a-f52c1e050835"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text in response_stream.response_gen:\n",
        "    print(text,end=\"\",flush=True)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHLqzn8jNZQn",
        "outputId": "ce4c89c6-ca1c-4052-db0e-8cb2816240f6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Based on the provided context information, it appears that you are interested in signing up for a program related to evidence log recording. To sign up, you can follow these steps:\n",
            "1. Carry your log with you or keep notebooks"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =   26398.82 ms\n",
            "llama_print_timings:      sample time =      29.96 ms /    50 runs   (    0.60 ms per token,  1668.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =  109232.92 ms /   251 tokens (  435.19 ms per token,     2.30 tokens per second)\n",
            "llama_print_timings:        eval time =   30782.38 ms /    49 runs   (  628.21 ms per token,     1.59 tokens per second)\n",
            "llama_print_timings:       total time =  252703.61 ms /   300 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_stream = query_engine.query(\"What Method of Counseling do you offer?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJfd7AVVOeYc",
        "outputId": "50e322f2-1106-4f1c-e646-bfadd0856466"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text in response_stream.response_gen:\n",
        "    print(text,end=\"\",flush=True)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnHXm_0sOjx_",
        "outputId": "cc4916e6-4686-4d6e-a974-65352346f43b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Based on the provided context information, MindWorks Buddy offers various methods of counseling, including:\n",
            "1. Online Counseling: You can connect with their expert psychologists through their website and receive immediate support.\n",
            "2"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =   26398.82 ms\n",
            "llama_print_timings:      sample time =      28.94 ms /    50 runs   (    0.58 ms per token,  1728.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =   31750.60 ms /    50 runs   (  635.01 ms per token,     1.57 tokens per second)\n",
            "llama_print_timings:       total time =   37115.83 ms /    50 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_stream = query_engine.query(\"What happens in the first session?\")"
      ],
      "metadata": {
        "id": "fi6Of1DeUEog"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in response_stream.response_gen:\n",
        "    print(text,end=\"\",flush=True)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq6Mx5LoUO_F",
        "outputId": "0ee356b1-9d76-416e-ddb1-22a34075ffda"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Based on the given context information, it appears that the individual is seeking guidance on how to book a counseling session with a chosen counselor. The process involves logging into a website, selecting the counselor of choice, choosing a date and time, and scheduling the session. Once the session is booked, the chosen counselor will contact the individual and provide a Google link for the video session.\n",
            "In terms of the first session, it is likely that the counselor will ask the individual questions to get to know them better and understand their reasons for seeking counseling. The counselor may also provide guidance on how to manage any challenges or issues that the individual is facing, and offer support and resources as needed. The exact content of the first session will depend on the individual's specific needs and goals, as well as the counselor's area of expertise."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =  145097.58 ms\n",
            "llama_print_timings:      sample time =     105.69 ms /   188 runs   (    0.56 ms per token,  1778.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =  145096.98 ms /   354 tokens (  409.88 ms per token,     2.44 tokens per second)\n",
            "llama_print_timings:        eval time =  117244.48 ms /   187 runs   (  626.98 ms per token,     1.59 tokens per second)\n",
            "llama_print_timings:       total time =  293084.37 ms /   541 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}